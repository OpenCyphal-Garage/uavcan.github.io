---
permalink: /Specification/6._Application_level_functions/
---

# Application-level functions

{% include lightbox.html url="/Specification/figures/architecture.png" title="UAVCAN architecture" thumbnail=true maxwidth="40%" %}

The higher level concepts of UAVCAN are described in this section.

## Node initialization

UAVCAN does not require that nodes undergo any specific initialization upon connecting to the bus -
a node is free to begin functioning immediately once it is powered up.
The only application-level function that every UAVCAN node must support is the periodic broadcasting of the
node status message, which is documented next.

## Node status reporting

Every UAVCAN node must report its status and presence by broadcasting messages of type
`uavcan.protocol.NodeStatus`.
This is the only data structure that UAVCAN nodes are required to support.
All other application-level functions are considered optional.

Note that the ID of this message contains a long sequence
of alternating 0 and 1 values when represented in binary, which facilitates automatic CAN bus bit rate detection.

The definition of the message is provided below.

{% include dsdl.md prefix='uavcan.protocol.NodeStatus' %}

## Node discovery

UAVCAN provides mechanisms to obtain the list of all nodes present in the network
as well as detailed information about each node:

- Lists of all nodes that are connected to the bus can be created and maintained
by listening for node status messages `uavcan.protocol.NodeStatus`.
- Extended information about each node can be requested using the services documented below.

Note that **it is highly recommended to support the service `uavcan.protocol.GetNodeInfo` in every node**,
as it is vital for node discovery and identification.

{% include dsdl.md prefix='uavcan.protocol.GetNodeInfo' %}

{% include dsdl.md prefix='uavcan.protocol.GetDataTypeInfo' %}

## Time synchronization

UAVCAN supports network-wide precise time synchronization with a resolution of up to 1 CAN bus bit period
(i.e., 1 microsecond for 1 Mbps CAN bit rate), assuming that CAN frame timestamping is supported by the hardware.
The algorithm can also function in the absence of hardware support for timestamping, although its performance will be
degraded.

The time synchronization approach is based on the work
"Implementing a Distributed High-Resolution Real-Time Clock using the CAN-Bus" (M. Gergeleit and H. Streich).
The general idea of the algorithm is to have one or more nodes that periodically broadcast a message of type
`uavcan.protocol.GlobalTimeSync` (definition is provided below) containing the exact timestamp of the previous
transmission of this message.
A node that performs a periodic broadcast of this message is referred to as a *time synchronization master*,
whereas a node that synchronizes its time with the master is referred to as a *time synchronization slave*.

Note that this algorithm only allows to precisely estimate the phase difference between the given slave and
the master it is synchronized with.
UAVCAN does not define the algorithm for clock speed/phase adjustment, which is entirely implementation defined.

The following constants are defined for the time synchronization algorithm:

* *T<sub>max</sub>* - maximum broadcast interval for a given master.
* *T<sub>min</sub>* - minimum broadcast interval for a given master.
* *T<sub>timeout</sub>* - if the master was not broadcasting the time synchronization message for this amount of time,
all slaves shall switch to the next active master with the highest priority.

The network may accommodate more than one time synchronization master working at the same time.
In this case, only the master with the lowest node ID should be active;
other masters should become passive by means of stopping broadcasting time synchronization messages,
and they must synchronize with the active master instead.
If the currently active master was not broadcasting time synchronization messages for the duration of
*T<sub>timeout</sub>*, the next master with the highest priority becomes active instead,
and all slaves will synchronize with it.
When a higher priority master appears in the network, all other lower-priority masters should become passive,
and all slaves will synchronize with the new master immediately.

The message `uavcan.protocol.GlobalTimeSync` contains the exact timestamp of the previous transmission of this message.
If the previous message was not yet transmitted,
or if it was transmitted more than *T<sub>max</sub>* time units ago, the field must be set to zero.

It is recommended to refer to the existing UAVCAN implementations for reference.

### Master

The following pseudocode describes the logic of a time synchronization master.

```cpp
// State variables:
transfer_id := 0;
previous_tx_timestamp[NUM_IFACES];
previous_broadcast_timestamp;

// This function broadcasts a message with a specified Transfer ID using only one iface:
function broadcastMessage(transfer_id, iface_index, msg);

// This function returns the current value of a monotonic clock (the clock that doesn't change phase or rate)
function getMonotonicTime();

// This callback is invoked when the CAN driver completes transmission of a time sync message
// The tx_timestamp argument contains the exact timestamp when the CAN frame was delivered to the bus
function messageTxTimestampCallback(iface_index, tx_timestamp)
{
    previous_tx_timestamp[iface_index] := tx_timestamp;
}

// Publishes the message of type uavcan.protocol.GlobalTimeSync to each available interface
function broadcastTimeSync()
{
    current_time := getMonotonicTime();

    if (current_time - previous_broadcast_timestamp < MIN_PUBLICATION_PERIOD)
    {
        return;     // Rate limiting
    }

    if (current_time - previous_broadcast_timestamp > MAX_PUBLICATION_PERIOD)
    {
        for (i := 0; i < NUM_IFACES; i++)
        {
            previous_tx_timestamp[i] := 0;
        }
    }

    previous_broadcast_timestamp := current_time;

    message := uavcan.protocol.GlobalTimeSync();

    for (i := 0; i < NUM_IFACES; i++)
    {
        message.previous_transmission_timestamp_usec := previous_tx_timestamp[i];
        previous_tx_timestamp[i] := 0;
        broadcastMessage(transfer_id, i, message);
    }

    transfer_id++; // Overflow must be handled correctly
}
```

### Slave

The following pseudocode describes the logic of a time synchronization slave.

```cpp
// State variables:
previous_rx_real_timestamp := 0;               // This time is being synchronized
previous_rx_monotonic_timestamp := 0;     // This is the monotonic time (doesn't jump or change rate)
previous_transfer_id := 0;
state := STATE_UPDATE;       // STATE_UPDATE, STATE_ADJUST
master_node_id := -1;        // Invalid value
iface_index := -1;           // Invalid value

// This function performs local clock adjustment:
function adjustLocalTime(phase_error);

function adjust(message)
{
    // Clock adjustment will be performed every second message
    local_time_phase_error := previous_rx_real_timestamp - msg.previous_transmission_timestamp_usec;
    adjustLocalTime(local_time_phase_error);
    state := STATE_UPDATE;
}

function update(message)
{
    // Message is assumed to have two timestamps:
    //   Real - sampled from the clock that is being synchronized
    //   Monotonic - clock that never jumps and never changes rate
    previous_rx_real_timestamp := message.rx_real_timestamp;
    previous_rx_monotonic_timestamp := message.rx_monotonic_timestamp;
    master_node_id := message.source_node_id;
    iface_index := message.iface_index;
    previous_transfer_id := message.transfer_id;
    state := STATE_ADJUST;
}

// Accepts the message of type uavcan.protocol.GlobalTimeSync (please refer to the DSDL definition)
function handleReceivedTimeSyncMessage(message)
{
    time_since_previous_msg := message.monotonic_timestamp - previous_rx_monotonic_timestamp;

    // Resolving the state flags:
    needs_init := (master_node_id < 0) or (iface_index < 0);
    switch_master := message.source_node_id < master_node_id;
    publisher_timed_out := time_since_previous_msg > PUBLISHER_TIMEOUT;

    if (needs_init or switch_master or publisher_timed_out)
    {
        update(message);
    }
    else if ((message.iface_index == iface_index) and (message.source_node_id == master_node_id))
    {
        // Revert the state to STATE_UPDATE if needed
        if (state == STATE_ADJUST)
        {
            msg_invalid := message.previous_transmission_timestamp_usec == 0;
            wrong_tid := message.transfer_id != (previous_transfer_id + 1);    // Overflow must be handled correctly
            wrong_timing := time_since_previous_msg > MAX_PUBLICATION_PERIOD;
            if (msg_invalid or wrong_tid or wrong_timing)
            {
                state := STATE_UPDATE;
            }
        }
        // Handle the current state
        if (state == STATE_ADJUST)
        {
            adjust(message);
        }
        else
        {
            update(message);
        }
    }
    else
    {
        ; // Ignore this message
    }
}
```

{% include dsdl.md prefix='uavcan.protocol.GlobalTimeSync' %}

## Node configuration

UAVCAN defines standard services for management of remote node's configuration parameters.
Support for these services is not mandatory but is highly recommended. The services are as follows:

* `uavcan.protocol.param.GetSet` - gets or sets a single configuration parameter value, either by name or by index.
* `uavcan.protocol.param.ExecuteOpcode` - allows control of the node configuration,
including saving the configuration into the non-volatile memory, or resetting the configuration to default settings.
* `uavcan.protocol.RestartNode` - restarts a node remotely.
Some nodes may require a restart before new configuration parameters can be applied.

In some cases, a node may require more complex configuration than can be conveniently managed via these services.
If this is the case, the recommendation is to manage the node's configuration through configuration files accessible via
the standard file management services (documented in this section).

{% include dsdl.md prefix='uavcan.protocol.param.' %}

### Standard configuration parameters

There are some configuration parameters that are common for most UAVCAN nodes.
Examples of such common parameters include message publication frequencies, non-default data type ID settings,
local node ID, etc.
The UAVCAN specification improves compatibility by providing the following naming
conventions for UAVCAN-related configuration parameters.
Following these conventions is highly encouraged, but not mandatory.

As can be seen below, all standard UAVCAN-related parameters share the same prefix `uavcan.`.

#### Data type ID

Parameter name: `uavcan.dtid-X`, where **X** stands for the full data type name, e.g.
`uavcan.dtid-uavcan.protocol.NodeStatus`.

This parameter configures the data type ID value for a given data type.

#### Message publication period

Parameter name: `uavcan.pubp-X`, where **X** stands for the full data type name; e.g.
`uavcan.pubp-uavcan.protocol.NodeStatus`.

This parameter configures the publication period for a given data type, in integer number of microseconds.
Zero value means that publication should be disabled.

#### Transfer priority

Parameter name: `uavcan.prio-X`, where **X** stands for the full data type name, e.g.
`uavcan.prio-uavcan.protocol.NodeStatus`.

This parameter configures the transport priority level that will be used when publishing messages or
calling services of a given data type.

#### Node ID

Parameter name: `uavcan.node_id`.

This parameter configures ID of the local node.
Zero means that the node ID is unconfigured, which may prompt the node to resort to dynamic node ID
allocation after startup.

#### CAN bus bit rate

Parameter name: `uavcan.bit_rate`.

This parameter configures CAN bus bit rate.
Zero value should trigger automatic bit rate detection, which should be the default option.
Please refer to the hardware design recommendations for recommended values and other details.

#### Instance ID

Parameter name: `uavcan.id-X-Y`, where **X** is namespace name; **Y** is ID field name.

Some UAVCAN messages (standard and possibly vendor-specific ones) use special fields that identify the instance of
a certain function - *ID fields*.
For example, messages related to actuator control use fields named `actuator_id`,
some sensor messages use fields named `sensor_id`, etc.
In order to improve compatibility, the specification offers a naming convention for parameters that define the
values used in ID fields.

Given messages located in the namespace **X** that share an ID field named **Y**,
the corresponding parameter name would be `uavcan.id-X-Y`.
For example, the parameter for the field `esc_index` that is used in the message `uavcan.equipment.esc.Status`
and that defines the array index in `uavcan.equipment.esc.RawCommand`, will be named as follows:

    uavcan.id-uavcan.equipment.esc-esc_index

In the case that an ID field is shared across different namespaces,
then the most common outer shared namespace should be used as **X**.
This is not the case for any of the standard messages, so an example cannot be provided.

In the case that an ID field is used in the standard namespace (`uavcan.*`) and in some vendor-specific namespaces
at the same time, the prefix should be used as though the ID field was used only in the standard namespace.

## File transfer

File transfer is a very generic feature of UAVCAN, that allows access to the file system on remote nodes.
The feature is based upon a set of UAVCAN services that are listed below.

### Firmware update

In terms of UAVCAN, firmware update is a special case of file transfer.
The process of firmware update involves two or three nodes:

* The node that initiates the process of firmware update, or *updater*.
* The node that provides access to the firmware file, or *file server*.
In most cases, the updater will be acting as a file server and only two nodes are involved.
* The node that is being updated, or *updatee*.

The process can be described as follows:

1. The updater decides that a certain node (the *updatee*) should be updated.
2. The updater invokes the service `uavcan.protocol.file.BeginFirmwareUpdate` on the updatee.
The information about the location of the firmware file will be passed to the updatee via the service request.
3. If the updatee chooses to accept the update request, it performs initialization procedures as required by its
implementation (e.g., rebooting into the bootloader, etc).
4. The updatee receives new firmware file from the file server using information received via the service request above.
5. The updatee completes the update and restarts.

Typically, the updatee will also resort to the dynamic node ID allocation process,
which is documented in this section.

{% include dsdl.md prefix='uavcan.protocol.file.' %}

## Debug features

The following messages are designed to facilitate debugging and to provide means of reporting events in
a human-readable representation.

{% include dsdl.md prefix='uavcan.protocol.debug.' %}

## Command shell access

The following service allows execution of arbitrary commands on a remote node via direct access to its
internal command shell.

{% include dsdl.md prefix='uavcan.protocol.AccessCommandShell' %}

## Panic mode

The panic message allows the broadcaster to quickly shut down the system in the event of an emergency.

{% include dsdl.md prefix='uavcan.protocol.Panic' %}

## Dynamic node ID allocation

In order to be able to operate in a UAVCAN network, a node must have a node ID that is unique within the network.
Typically, a valid node ID can be configured manually for each node; however, in certain use cases the manual
approach is either undesirable or impossible, therefore UAVCAN defines the high-level feature of dynamic node ID
allocation, that allows nodes to obtain a node ID value automatically upon connection to the network.

Dynamic node ID allocation combined with automatic CAN bus bit rate detection makes it easy to
implement nodes that can join any UAVCAN network without any manual configuration.
These sorts of nodes are referred to as *plug-and-play nodes*.

A dynamically allocated node ID cannot be persistent.
This means that if a node is configured to use a dynamic node ID,
it *must* perform a new allocation every time it starts or reboots.

The process of dynamic node ID allocation always involves two types of nodes: *allocators*, which serve
allocation requests; and *allocatees*, which request dynamic node ID from allocators.
A UAVCAN network may implement the following configurations of allocators:

* Zero allocators, in which case the feature of dynamic node ID allocation will not be available.
* One allocator, in which case the feature of dynamic node ID allocation will become unavailable if the allocator fails.
In this configuration, the role of the allocator can be performed even by a very
resource-constrained system, e.g. a low-end microcontroller.
* Three allocators, in which case the allocators will be using a replicated state via a distributed consensus
algorithm. In this configuration, the network can tolerate the loss of one allocator and continue to serve allocation
requests. This configuration requires that the allocators to maintain large data structures for the purposes of the
distributed consensus algorithm, and may therefore require a slightly more sophisticated computational platform,
e.g. a high-end microcontroller.
* Five allocators, is the same as the three allocator configuration except that the network can tolerate the loss of two
allocators and still continue to serve allocation requests.

In order to get a dynamic node ID, each allocatee must have a globally unique 128-bit integer identifier,
known as *unique ID*.
This is the same value that is used in the field `unique_id` of the data type `uavcan.protocol.HardwareVersion`.
Every node that requires a dynamic ID allocation must support the service `uavcan.protocol.GetNodeInfo`,
and the nodes must use the same unique ID value during dynamic node ID allocation
and when responding to `uavcan.protocol.GetNodeInfo` requests.

During dynamic allocation, the allocatee communicates its unique ID to the allocator (or allocators),
which then use it to produce an appropriate allocation response.
Unique ID values are kept by allocators in *allocation tables* -
data structures that contain mappings between unique ID and corresponding node ID values.
Allocation tables are write-only data structures that can only grow. Once a new allocatee has requested a node ID,
its unique ID will be recorded into the allocation table,
and all subsequent allocation requests from the same allocatee will be served with the same node ID value.

In configurations with redundant allocators, every allocator maintains a replica of the same allocation table
(a UAVCAN network cannot contain more than one allocation table, regardless of the number of allocators employed).
While the allocation table is write-only data structure that can only grow,
it is still possible to wipe the table completely, forcing the allocators to
forget known nodes and perform all following allocations anew.

In the context of this chapter, nodes that are using dynamic node ID will be referred to as *dynamic nodes*, and
nodes that are using manually-configured node ID will be referred to as *static nodes*.
It is assumed that in most cases, allocators will be static nodes themselves (since there's no other authority on the
network that can grant dynamic node ID, allocators will not be able to dynamically allocate themselves).
Excepting allocators, it is not recommended to mix dynamic and static nodes on the same network; i.e.,
normally, a UAVCAN network should contain either all static nodes, or all dynamic nodes (except allocators).
In case if this recommendation cannot be followed,
the following rules of safe co-existence of dynamic nodes with static nodes must be considered:

1. It is safe to connect dynamic nodes to the bus at any time.
2. A static node can be connected to the bus if the allocator (allocators) is (are) already aware of them,
i.e. these static nodes are already in the allocation table.
3. A new static node (i.e. a node that does not meet the above condition) can be connected to the bus only if:
   1. New dynamic allocations are not happening at the moment.
   2. The allocators are capable of serving new allocations.

As can be inferred from the above, the process of dynamic node ID allocation involves up to two types of
communications:

* *Allocatee-allocator* - this communication is used when an allocatee requests a dynamic node ID from the allocator
(allocators), and when the allocator (allocators) transmits a response back to the allocatee.
This communication is invariant to the allocator configuration used, i.e., the allocatees are not aware of
how many allocators are available on the network and how they are configured.
* *Allocator-allocator* - this communication is used by allocators for the purpose of maintenance of the
replicated allocation table and for other needs of the distributed consensus algorithm. Allocatees are completely
isolated and unaware of these exchanges. This communication is not applicable for the single-allocator configuration.

### Allocatee-allocator exchanges

Allocatee-allocator exchanges are performed using only one message type -
`uavcan.protocol.dynamic_node_id.Allocation`.
Allocators use it with regular message broadcast transfers; allocatees use it with anonymous message transfers.
The specification and usage info for this data type is provided below.

The general idea of the allocatee-allocator exchanges is that the allocatee communicates to the allocator its
unique ID and, if applicable, the preferred node ID value, using anonymous message transfers of type
`uavcan.protocol.dynamic_node_id.Allocation`.
The allocator performs the allocation and sends a response using the same message type, where the field for
unique ID is populated with the unique ID of the requesting node and the field for node ID is populated with the
allocated node ID.
Note that since the allocator that serves the allocation always has a node ID, it is free to use multi-frame transfers,
therefore the allocator can directly send the response using a single message transfer.
The allocatees, however, are restricted to single-frame transfers, due to limitations of anonymous message transfers.
Therefore, the allocatees send their unique ID to the allocator using three single-frame transfers, where the first
transfer contains the first part of their unique ID, second transfer contains the continuation, and the last transfer
contains the last few bytes of the unique ID.
The details are provided in the DSDL description of the message type.

The specification of the data type contains a description of the exchange protocol on the side of allocatee.
On the allocator's side the algorithm should be implemented as shown in the following pseudocode.

Please note that the pseudocode refers to a function named `canPublishFollowupAllocationResponse()`,
which is only applicable in the case of a redundant allocator configuration.
It evaluates the current state of the distributed consensus and decides whether the current node
is allowed to engage in allocation exchanges.
The logic of this function will be reviewed in the chapter dedicated to redundant allocators.
In the non-redundant allocator configuration, this function will always return true,
meaning that the allocator is always allowed to engage in allocation exchanges.

```cpp
// Constants:
InvalidStage = 0;

// State variables:
last_message_timestamp;
current_unique_id;

// This function will be invoked when a complete unique ID is received.
// Typically, the actual allocation will be carried out in this function.
function handleAllocationRequest(unique_id, preferred_node_id);

// This function is only applicable in a configuration with redundant allocators.
// Its return value depends on the current state of the distributed consensus algorithm.
// Please refer to the allocator-allocator communication logic for details.
// In the non-redundant configuration this function will always return true.
function canPublishFollowupAllocationResponse();

// This is an internal function; see below.
function detectRequestStage(msg)
{
    if ((msg.unique_id.size() != MAX_LENGTH_OF_UNIQUE_ID_IN_REQUEST) &&
        (msg.unique_id.size() != (msg.unique_id.capacity() - MAX_LENGTH_OF_UNIQUE_ID_IN_REQUEST * 2U)) &&
        (msg.unique_id.size() != msg.unique_id.capacity()))     // For CAN FD
    {
        return InvalidStage;
    }
    if (msg.first_part_of_unique_id)
    {
        return 1;       // Note that CAN FD frames can deliver the unique ID in one stage!
    }
    if (msg.unique_id.size() == MAX_LENGTH_OF_UNIQUE_ID_IN_REQUEST)
    {
        return 2;
    }
    if (msg.unique_id.size() < MAX_LENGTH_OF_UNIQUE_ID_IN_REQUEST)
    {
        return 3;
    }
    return InvalidStage;
}

// This is an internal function; see below.
function getExpectedStage()
{
    if (current_unique_id.empty())
    {
        return 1;
    }
    if (current_unique_id.size() >= (MAX_LENGTH_OF_UNIQUE_ID_IN_REQUEST * 2))
    {
        return 3;
    }
    if (current_unique_id.size() >= MAX_LENGTH_OF_UNIQUE_ID_IN_REQUEST)
    {
        return 2;
    }
    return InvalidStage;
}

// This function is invoked when the allocator receives a message of type uavcan.protocol.dynamic_node_id.Allocation.
function handleAllocation(msg)
{
    if (!msg.isAnonymousTransfer())
    {
        return;         // This is a response from another allocator, ignore
    }

    // Reset the expected stage on timeout
    if (msg.getMonotonicTimestamp() > (last_message_timestamp + FOLLOWUP_TIMEOUT))
    {
        current_unique_id.clear();
    }

    // Checking if request stage matches the expected stage
    request_stage = detectRequestStage(msg);
    if (request_stage == InvalidStage)
    {
        return;             // Malformed request - ignore without resetting
    }

    if (request_stage != getExpectedStage())
    {
        return;             // Ignore - stage mismatch
    }

    if (msg.unique_id.size() > current_unique_id.capacity() - current_unique_id.size())
    {
        return;             // Malformed request
    }

    // Updating the local state
    for (i = 0; i < msg.unique_id.size(); i++)
    {
        current_unique_id.push_back(msg.unique_id[i]);
    }

    if (current_unique_id.size() == current_unique_id.capacity())
    {
        // Proceeding with allocation.
        handleAllocationRequest(current_unique_id, msg.node_id);
        current_unique_id.clear();
    }
    else
    {
        // Publishing the follow-up if possible.
        if (canPublishFollowupAllocationResponse())
        {
            msg = uavcan.protocol.dynamic_node_id.Allocation();
            msg.unique_id = current_unique_id;
            broadcast(msg);
        }
        else
        {
            current_unique_id.clear();
        }
    }

    // It is important to update the timestamp only if the request has been processed successfully.
    last_message_timestamp = msg.getMonotonicTimestamp();
}
```

{% include dsdl.md prefix='uavcan.protocol.dynamic_node_id.Allocation' header_prefix='####' %}

The following diagram may aid understanding of the allocatee side of the algorithm (click to enlarge):

{% include lightbox.html url="/Specification/figures/dynamic_node_id_allocatee_algorithm.svg" title="Dynamic node ID allocation algorithm" maxwidth="75%" %}

#### Example

The following log provides a real-world example of a dynamic node ID allocation process:

    Time   CAN ID     CAN data field
    1.117  1EEE8100   01 44 C0 8B 63 5E 05 C0
    1.117  1E000101   00 44 C0 8B 63 5E 05 C0
    1.406  1EEBE500   00 F4 BC 10 96 DF 11 C1
    1.406  1E000101   05 B0 00 44 C0 8B 63 81
    1.406  1E000101   5E 05 F4 BC 10 96 DF 21
    1.406  1E000101   11 41
    1.485  1E41E100   00 A8 BA 54 47 C2
    1.485  1E000101   29 BA FA 44 C0 8B 63 82
    1.485  1E000101   5E 05 F4 BC 10 96 DF 22
    1.485  1E000101   11 A8 BA 54 47 42

First, the allocatee waits for a random time interval in order to ensure that other allocations are not happening
at the moment.
After the delay, the allocatee announces its intention to get a node ID allocation by broadcasting the
following anonymous message:

    1.117  1EEE8100   01 44 C0 8B 63 5E 05 C0

The allocator responds immediately with confirmation:

    1.117  1E000101   00 44 C0 8B 63 5E 05 C0

The allocatee waits for another random time interval in order to ensure that it will not conflict with
other nodes that have unique node ID with the same first six bytes.
After the delay, the allocatee sends the second-stage request:

    1.406  1EEBE500   00 F4 BC 10 96 DF 11 C1

The allocator responds immediately with confirmation.
This time, the confirmation contains 12 bytes of unique ID, so it doesn't fit one CAN frame,
therefore the allocator resorts to a multi-frame transfer:

    1.406  1E000101   05 B0 00 44 C0 8B 63 81
    1.406  1E000101   5E 05 F4 BC 10 96 DF 21
    1.406  1E000101   11 41

The allocatee waits for another random time interval in order to ensure that it will not conflict with
other nodes that have unique node ID with the same first twelve bytes.
After the delay, the allocatee sends the third-stage request:

    1.485  1E41E100   00 A8 BA 54 47 C2

At this moment the allocator has received full unique ID and the preferred node ID of the allocatee as well.
The allocator can carry out the actual allocation and send a response:

    1.485  1E000101   29 BA FA 44 C0 8B 63 82
    1.485  1E000101   5E 05 F4 BC 10 96 DF 22
    1.485  1E000101   11 A8 BA 54 47 42

This completes the process.
Next time the allocatee sends an allocation request, it will be provided with the same node ID.

The values used in the example above were the following:

Name                | Value
--------------------|--------------------------------------------------------------------------------------------------
Unique ID           | `44 C0 8B 63 5E 05 F4 BC 10 96 DF 11 A8 BA 54 47` (hex)
Preferred node ID   | 0 (any)
Allocated node ID   | 125

### Non-redundant allocator

UAVCAN does not impose specific requirements to the implementation of a non-redundant allocator,
except its duties listed below.

#### Duties of the allocator

The allocator is tasked with monitoring the nodes present in the network.
When a new node appears, the allocator must invoke `uavcan.protocol.GetNodeInfo` on it,
and check the received unique ID against the allocation table.
If a matching entry is not found in the table, the allocator will create one.
If the node failed to respond to `uavcan.protocol.GetNodeInfo` after at least 3 attempts,
the allocator will extend the allocation table with a *mock entry*,
where the node ID is matching the real node ID of the non-responding node, and unique ID is set to zero.
This ensures that dynamic nodes will not be granted a node ID value that is already taken by a static node.
This requirement demonstrates why is it mandatory that dynamic nodes use the same unique ID
both when responding to `uavcan.protocol.GetNodeInfo` and when publishing allocation requests.

### Redundant allocators

The algorithm used for replication of the allocation table across redundant allocators is a fairly direct
implementation of the [Raft consensus algorithm](https://raft.github.io/), as published in the paper
"In Search of an Understandable Consensus Algorithm (Extended Version)" (Diego Ongaro and John Ousterhout).
The following text assumes that the reader is familiar with the paper.

#### Raft log

The *Raft log* contains entries of type `uavcan.protocol.dynamic_node_id.server.Entry` (defined below),
where every entry contains Raft term number, unique ID, and the matching node ID value.
Therefore, the raft log is the allocation table itself.

Since the maximum number of entries in the allocation table is limited by the range of node ID,
the log cannot contain more than 127 entries.
Therefore, snapshot transfer and log compaction are not required, so they are not implemented in the algorithm.

When a server becomes the leader, it checks if the Raft log contains an entry for its own unique ID, and if it doesn't,
the leader adds its own allocation entry to the log.
This feature guarantees that the raft log always contains at least one entry,
therefore it is not necessary to support negative log indices, as proposed by the Raft paper.

Since the log is write-only and limited in growth, all allocations are permanent.
This restriction is acceptable, since UAVCAN is a vehicle bus, and configuration of vehicle's components is not
expected to change frequently.
Old allocations can be removed in order to free node IDs for new allocations, by clearing the Raft log
on all allocators.

#### Cluster configuration

The allocators need to be aware of each other's node ID in order to form a cluster.
In order to learn each other's node ID values, the allocators broadcast messages of type
`uavcan.protocol.dynamic_node_id.server.Discovery` (defined below) until the cluster is fully discovered.

This extension to the Raft algorithm makes the cluster almost configuration-free - the only parameter that must be
configured on all servers of the cluster is the number of nodes in the cluster (everything else will be auto-detected).

Runtime cluster membership changes are not supported, since they are not needed for a vehicle bus.

#### Duties of the leader

The leader is tasked with monitoring the nodes present in the network.
Please refer to the section dedicated to duties of a non-redundant allocator for details.

Only the leader can process allocation requests and engage in communication with allocatees.
An allocator is allowed to send allocation responses only if both conditions are met:

* The allocator is a leader.
* Its replica of the Raft log does not contain uncommitted entries
(i.e. the last allocation request has been completed successfully).

The second condition needs to be explained by an example.

Consider a case with two Raft nodes that are residing in different network partitions,
unable to communicate with each other - **A** and **B**, both of them are leaders;
**A** can commit to the log, and **B** is in a minor partition.
Then there is an allocatee **X** that can exchange with both leaders,
and an allocatee **Y** that can exchange only with **A**.
Such a situation can occur as a result of a specific failure mode of redundant interfaces.

Both allocatees **X** and **Y** initially send first-stage allocation requests;
**A** responds to **Y** with a first-stage response, whereas **B** responds to **X**.
Both **X** and **Y** will issue follow-up requests,
which may cause **A** to mix allocation requests from different nodes,
leading to reception of an invalid unique ID.
When both leaders receive full unique ID values
(**A** will receive an invalid one, and **B** will receive a valid unique ID of **X**),
only **A** will be able to make a commit, because **B** is in a minor partition.
Since both allocatees were unable to receive node ID values in this round, they will retry later.

Now, in order to prevent **B** from disrupting allocatee-allocator communication again,
we introduce this second restriction:
an allocator cannot exchange with allocatees as long as its log contains uncommitted entries.

Note that this restriction does not apply to allocation requests sent via CAN FD frames
as these allow larger frames such that all necessary information to be exchanged in a single request and response.
Only CAN FD can offer perfectly reliable allocation exchanges.

{% include dsdl.md prefix='uavcan.protocol.dynamic_node_id.server' header_prefix='####' %}

#### Example

The following log demonstrates relevant messages that were transferred over the CAN bus in the process of
a dynamic node ID allocation for one allocatee, where the allocators were running a three-node Raft cluster.
All node status messages were removed for clarity.

The configuration was as follows:

Name                            | Value
--------------------------------|--------------------------------------------------------------------------------------
Number of allocators            | 3
Allocators' node ID             | 1, 2, 3
Leader's node ID                | 1
Allocatee's unique ID           | `44 C0 8B 63 5E 05 F4 BC 83 3B 3A 88 1C 43 60 50` (hex)
Preferred node ID               | 0 (any)
Allocated node ID               | 125
Discovery broadcasting interval | 1 second
AppendEntries interval          | 1 second per follower


    Time   CAN ID     CAN data field
    0.000  1E018601   03 01 C0
    0.512  1E018602   03 02 01 C0
    0.905  1E018603   03 03 01 02 C0
    1.000  1E018601   03 01 02 03 C1
    1.512  1E018602   03 02 01 03 C1
    <cluster maintenance traffic omitted for clarity>
    2.569  1EEE8100   01 44 C0 8B 63 5E 05 C0
    2.569  1E000101   00 44 C0 8B 63 5E 05 C0
    2.684  1E238D00   00 F4 BC 83 3B 3A 88 C1
    2.684  1E000101   5C EF 00 44 C0 8B 63 81
    2.684  1E000101   5E 05 F4 BC 83 3B 3A 21
    2.684  1E000101   88 41
    2.756  1E1E8381   5F CF 2E 00 00 00 04 85
    2.756  1E1E8381   00 00 00 05 05 65
    2.756  1E1E0183   2E 00 00 00 80 C5
    2.871  1E63ED00   00 1C 43 60 50 C2
    3.256  1E1E8281   9C 38 2E 00 00 00 04 87
    3.256  1E1E8281   00 00 00 05 05 2E 00 27
    3.256  1E1E8281   00 00 44 C0 8B 63 5E 07
    3.256  1E1E8281   05 F4 BC 83 3B 3A 88 27
    3.256  1E1E8281   1C 43 60 50 7D 47
    3.258  1E1E0182   2E 00 00 00 80 C7
    3.563  1E2F0D00   01 44 C0 8B 63 5E 05 C3
    3.756  1E1E8381   9C 38 2E 00 00 00 04 86
    3.756  1E1E8381   00 00 00 05 05 2E 00 26
    3.756  1E1E8381   00 00 44 C0 8B 63 5E 06
    3.756  1E1E8381   05 F4 BC 83 3B 3A 88 26
    3.756  1E1E8381   1C 43 60 50 7D 46
    3.756  1E000101   C7 36 FA 44 C0 8B 63 82
    3.756  1E000101   5E 05 F4 BC 83 3B 3A 22
    3.756  1E000101   88 1C 43 60 50 42
    3.758  1E1E0183   2E 00 00 00 80 C6
    4.256  1E1E8281   65 19 2E 00 00 00 2E 88
    4.256  1E1E8281   00 00 00 06 06 68
    4.256  1E1E0182   2E 00 00 00 80 C8
    4.756  1E1E8381   65 19 2E 00 00 00 2E 87
    4.756  1E1E8381   00 00 00 06 06 67
    4.756  1E1E0183   2E 00 00 00 80 C7

Once the first node of the cluster has been started,
it has published a cluster discovery message so it could become aware of its siblings,
and other two nodes that were started a fraction of a second later did the same:

    0.000  1E018601   03 01 C0

    0.512  1E018602   03 02 01 C0

    0.905  1E018603   03 03 01 02 C0

    1.000  1E018601   03 01 02 03 C1

    1.512  1E018602   03 02 01 03 C1

It can be seen that the last two discovery messages contain complete list of all nodes in the cluster.
The allocators have detected the fact that all nodes in the cluster were now aware of each other,
and ceased to broadcast discovery messages in order to not pollute the bus with redundant traffic.

Afterwards, the allocators ran elections and have elected the node 1 as their leader.
The leader then performed a few AppendEntries calls in order to synchronize the replicated log.
These exchanges are not shown for the sake of clarity.

The allocatee has appeared on the bus and has published first-stage and second-stage allocation requests.
The current leader was in charge with communicating with allocatee, other two allocators were silent:

    2.569  1EEE8100   01 44 C0 8B 63 5E 05 C0   <-- First stage request

    2.569  1E000101   00 44 C0 8B 63 5E 05 C0   <-- First stage response

    2.684  1E238D00   00 F4 BC 83 3B 3A 88 C1   <-- Second stage request

    2.684  1E000101   5C EF 00 44 C0 8B 63 81   <-- Second stage response
    2.684  1E000101   5E 05 F4 BC 83 3B 3A 21
    2.684  1E000101   88 41

While the allocatee was waiting for expiration of the random timeout,
the leader has performed a keep-alive AppendEntries call to the allocator 3:

    2.756  1E1E8381   5F CF 2E 00 00 00 04 85   <-- Empty AppendEntries request
    2.756  1E1E8381   00 00 00 05 05 65

    2.756  1E1E0183   2E 00 00 00 80 C5         <-- AppendEntries response

Then the allocatee has broadcasted the third-stage allocation request:

    2.871  1E63ED00   00 1C 43 60 50 C2

At this point the leader had the full unique ID of the allocatee,
so it has started the process of allocation and log replication.
In order to complete the allocation, the leader had to replicate the new entry of the Raft log
to a majority of allocators (see the Raft paper for details):

    3.256  1E1E8281   9C 38 2E 00 00 00 04 87   <-- AppendEntries request with new allocation
    3.256  1E1E8281   00 00 00 05 05 2E 00 27
    3.256  1E1E8281   00 00 44 C0 8B 63 5E 07
    3.256  1E1E8281   05 F4 BC 83 3B 3A 88 27
    3.256  1E1E8281   1C 43 60 50 7D 47

    3.258  1E1E0182   2E 00 00 00 80 C7         <-- AppendEntries response with confirmation

It can be seen that the follower took 2 milliseconds to update its persistent storage.

While the leader was busy replicating the allocation table
(it could not complete the allocation until the new log entry was committed),
the allocatee has given up waiting for a response and decided to restart the process.
This is not an error condition, but a normal behavior.
This time the leader did not engage in communication with the allocatee,
because the Raft log contained uncommitted entries.

    3.563  1E2F0D00   01 44 C0 8B 63 5E 05 C3   <-- First stage request, no response from the leader

Some time later the leader decided to replicate the new log entry to the other follower:

    3.756  1E1E8381   9C 38 2E 00 00 00 04 86   <-- AppendEntries request with new allocation
    3.756  1E1E8381   00 00 00 05 05 2E 00 26
    3.756  1E1E8381   00 00 44 C0 8B 63 5E 06
    3.756  1E1E8381   05 F4 BC 83 3B 3A 88 26
    3.756  1E1E8381   1C 43 60 50 7D 46

Immediately afterwards, the leader has noticed that the new entry has already been replicated
to a majority of allocators, therefore (see the Raft paper) the commit index could be incremented,
which completed the allocation.
Having detected that, the leader has published the allocation response:

    3.756  1E000101   C7 36 FA 44 C0 8B 63 82
    3.756  1E000101   5E 05 F4 BC 83 3B 3A 22
    3.756  1E000101   88 1C 43 60 50 42

While the leader was engaged in communications with the allocatee,
the follower 3 has finished updating its persistent storage and responded with confirmation:

    3.758  1E1E0183   2E 00 00 00 80 C6

At this moment the process was finished.
The leader then continued to invoke keep-alive AppendEntries calls to the followers:

    4.256  1E1E8281   65 19 2E 00 00 00 2E 88   <-- Empty AppendEntries request
    4.256  1E1E8281   00 00 00 06 06 68

    4.256  1E1E0182   2E 00 00 00 80 C8         <-- AppendEntries response

    4.756  1E1E8381   65 19 2E 00 00 00 2E 87   <-- Empty AppendEntries request
    4.756  1E1E8381   00 00 00 06 06 67

    4.756  1E1E0183   2E 00 00 00 80 C7         <-- AppendEntries response
